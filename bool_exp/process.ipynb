{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentDir = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict, Counter\n",
    "from string import punctuation\n",
    "import os\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_document(content):\n",
    "    \n",
    "    sentences = sent_tokenize(content)\n",
    "    tokens = []\n",
    "    for _sent in sentences:\n",
    "        sent_tokens = tokenizer.tokenize(_sent)\n",
    "        sent_tokens = [_tok.lower() for _tok in sent_tokens if _tok not in punctuation]\n",
    "        tokens += sent_tokens\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linguistic modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are no linguistic modules yet\n",
    "Possible are:\n",
    "- stemmer\n",
    "- lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(content):\n",
    "    return tokenize_document(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(documents_dir):\n",
    "\n",
    "    tokenized_documents = []\n",
    "    for document in os.listdir(documents_dir):\n",
    "        with open(os.path.join(documents_dir, document), errors='ignore',encoding='utf8') as outf:\n",
    "            tokenized_documents.append(preprocess_document(outf.read()))\n",
    "    print(\"Found documents: \", len(tokenized_documents))\n",
    "    \n",
    "    return tokenized_documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found documents:  3\n",
      "[['one', 'one'], ['two', 'one', 'two', 'two'], ['two', 'tree']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_documents = prepare_dataset(documentDir)\n",
    "print(tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import scandir # can be used for easier iteration of documents in a folder\n",
    "# can check is_file() on the objects returned by scan_dir \n",
    "# contain whole document path, so no need to join with the directory\n",
    "\n",
    "def get_token_doc_id_pairs(category_dir):\n",
    "\n",
    "    token_docid = []\n",
    "    doc_ids = {}\n",
    "\n",
    "    for i, document in enumerate(scandir(category_dir)):\n",
    "        if document.is_file():\n",
    "            doc_ids[i] = document.name\n",
    "            with open(document,encoding='utf8') as out_fp:\n",
    "                document_tokens = preprocess_document(out_fp.read())\n",
    "                token_docid += [(token, i) for token in document_tokens]\n",
    "    return token_docid, doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'one.txt', 1: 'one_two.txt', 2: 'two_tree.txt'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('one', 0),\n",
       " ('one', 0),\n",
       " ('two', 1),\n",
       " ('one', 1),\n",
       " ('two', 1),\n",
       " ('two', 1),\n",
       " ('two', 2),\n",
       " ('tree', 2)]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_docid, doc_ids = get_token_doc_id_pairs(documentDir)\n",
    "print(doc_ids)\n",
    "token_docid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by tokens to form the dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 0),\n",
       " ('one', 0),\n",
       " ('one', 1),\n",
       " ('tree', 2),\n",
       " ('two', 1),\n",
       " ('two', 1),\n",
       " ('two', 1),\n",
       " ('two', 2)]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "sorted_token_docid = sorted(token_docid, key=itemgetter(0))\n",
    "sorted_token_docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqash_token_in_doc(sorted_token_docid):\n",
    "    \"\"\"\n",
    "    Returns a list of (token, doc_id, term_freq) tuples from a sorted list of (token, doc_id) list, \n",
    "    where if a token appears n times in a doc_id, we merge it in a tuple (toke, doc_id, n).\n",
    "    \"\"\"\n",
    "    merged_tokens_in_doc = []\n",
    "    for token, doc_id in sorted_token_docid:\n",
    "        if merged_tokens_in_doc:\n",
    "            prev_tok, prev_doc_id, prev_freq = merged_tokens_in_doc[-1]\n",
    "            if prev_tok == token and prev_doc_id == doc_id:     \n",
    "                merged_tokens_in_doc[-1] = (token, doc_id, prev_freq+1)\n",
    "            else:\n",
    "                merged_tokens_in_doc.append((token, doc_id, 1))\n",
    "        else:\n",
    "            merged_tokens_in_doc.append((token, doc_id, 1))\n",
    "    return merged_tokens_in_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 0, 2), ('one', 1, 1), ('tree', 2, 1), ('two', 1, 3), ('two', 2, 1)]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_tokens_in_doc = sqash_token_in_doc(sorted_token_docid)\n",
    "merged_tokens_in_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dictionary = defaultdict(lambda: (0, 0)) # term : doc_freq, tot freq\n",
    "postings = defaultdict(lambda: []) # term: doc_ids, doc_freq\n",
    "\n",
    "for token, doc_id, doc_freq in merged_tokens_in_doc:\n",
    "    dictionary[token] = (dictionary[token][0]+1, dictionary[token][1\n",
    "    ]+doc_freq)\n",
    "\n",
    "# usually implemented as linked lists\n",
    "for token, doc_id, doc_freq in merged_tokens_in_doc:\n",
    "    postings[token].append((doc_id, doc_freq)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'one.txt', 1: 'one_two.txt', 2: 'two_tree.txt'}"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3), (2, 4), (1, 1), (0, 0))"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"one\"],dictionary['two'],dictionary['tree'],dictionary['zero']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 2), (1, 1)], [(1, 3), (2, 1)], [(2, 1)], [])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings[\"one\"],postings['two'],postings['tree'],postings['zero']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Opearations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_query(postings_word1, postings_word2):\n",
    " \n",
    "    documents_results = []\n",
    "    \n",
    "    postings_ind1, postings_ind2 = 0, 0\n",
    "    while postings_ind1 < len(postings_word1) and postings_ind2 < len(postings_word2):\n",
    "        doc_id1, doc_id2 = postings_word1[postings_ind1][0], postings_word2[postings_ind2][0]\n",
    "        if doc_id1 == doc_id2:\n",
    "            documents_results.append((doc_id1,0))\n",
    "            postings_ind1 += 1\n",
    "            postings_ind2 += 1\n",
    "        elif doc_id1 < doc_id2:\n",
    "            postings_ind1 += 1\n",
    "        elif doc_id1 > doc_id2:\n",
    "            postings_ind2 += 1\n",
    "    return documents_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_query(postings_word1, postings_word2):\n",
    "\n",
    "    documents_results = []\n",
    "    \n",
    "    postings_ind1, postings_ind2 = 0, 0\n",
    "    while postings_ind1 < len(postings_word1) and postings_ind2 < len(postings_word2):\n",
    "        doc_id1, doc_id2 = postings_word1[postings_ind1][0], postings_word2[postings_ind2][0]\n",
    "        if doc_id1 == doc_id2:\n",
    "            documents_results.append((doc_id1,0))\n",
    "            postings_ind1 += 1\n",
    "            postings_ind2 += 1\n",
    "        elif doc_id1 < doc_id2:\n",
    "            documents_results.append((doc_id1,0))\n",
    "            postings_ind1 += 1\n",
    "        elif doc_id1 > doc_id2:\n",
    "            documents_results.append((doc_id2,0))\n",
    "            postings_ind2 += 1\n",
    "    if postings_ind1 == len(postings_word1):\n",
    "        for i in range(postings_ind2,len(postings_word2)):\n",
    "            documents_results.append((postings_word2[i][0],0))\n",
    "    if postings_ind2 == len(postings_word2):\n",
    "        for i in range(postings_ind1,len(postings_word1)):\n",
    "            documents_results.append((postings_word1[i][0],0))\n",
    "    return documents_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_query(postings_word):\n",
    "    document_count = len(tokenized_documents)\n",
    "    documents_results = []\n",
    "\n",
    "    prev = 0\n",
    "    for i in range(0,len(postings_word)):\n",
    "        for not_doc in range(prev,postings_word[i][0]):\n",
    "            documents_results.append((not_doc,0))\n",
    "        prev = postings_word[i][0]+1\n",
    "    for not_doc in range(prev,document_count):\n",
    "        documents_results.append((not_doc,0))\n",
    "    \n",
    "    return documents_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0)]\n",
      "[(0, 0)]\n",
      "[(0, 0), (1, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(not_query(postings[\"one\"]))\n",
    "print(not_query(postings[\"two\"]))\n",
    "print(not_query(postings[\"tree\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parsing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "class BoolRetrievalOperand:\n",
    "    def __init__(self, t):\n",
    "        self.label = t[0]\n",
    "        print(\"Creating BoolRetrievalOperand\" + str(t))\n",
    "    \n",
    "    def process(self) -> list:\n",
    "        print(\"Processing BoolRetrievalOperand \"+ self.label)\n",
    "        self.value = postings[self.label]\n",
    "        print(self.value)\n",
    "        return self.value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.label\n",
    "\n",
    "    __repr__ = __str__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoolRetrievalNot:\n",
    "    def __init__(self, t):\n",
    "        print(\"Creating BoolRetrievalNot\"+str(t))\n",
    "        self.arg = t[0][1]\n",
    "        print(self.arg)\n",
    "\n",
    "\n",
    "    def process(self) -> list:\n",
    "        res = not_query(self.arg.process())\n",
    "        print(\"Processing \"+str(self))\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"~\" + str(self.arg)\n",
    "\n",
    "    __repr__ = __str__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoolRetrievalBinOp:\n",
    "    repr_symbol: str = \"\"\n",
    "    eval_fn: Callable[\n",
    "        [Iterable[list]], list\n",
    "    ] = lambda _: []\n",
    "\n",
    "    def __init__(self, t):\n",
    "        print(\"Creating BoolRetrievalBinOp \"+self.repr_symbol)\n",
    "        print(t)\n",
    "        self.args = t[0][0::2]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        sep = \" %s \" % self.repr_symbol\n",
    "        return \"(\" + sep.join(map(str, self.args)) + \")\"\n",
    "\n",
    "\n",
    "    def process(self) -> list:\n",
    "        res = self.eval_fn(a.process() for a in self.args)\n",
    "        print(\"Processing \"+str(self))\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "def and_helper(a,bb) -> list:\n",
    "    b = []\n",
    "    for i in bb:\n",
    "        b.append(i)\n",
    "    prev = b[0]\n",
    "    for i in range(0,len(b)-1):\n",
    "        prev = and_query(prev,b[i+1])\n",
    "    return prev\n",
    "\n",
    "def or_helper(a,bb) -> list:\n",
    "    b = []\n",
    "    for i in bb:\n",
    "        b.append(i)\n",
    "    prev = b[0]\n",
    "    for i in range(0,len(b)-1):\n",
    "        print(prev)\n",
    "        prev = or_query(prev,b[i+1])\n",
    "    return prev\n",
    "\n",
    "class BoolRetrievalAnd(BoolRetrievalBinOp):\n",
    "    repr_symbol = \"&\"\n",
    "    eval_fn = and_helper\n",
    "\n",
    "\n",
    "class BoolRetrievalOr(BoolRetrievalBinOp):\n",
    "    repr_symbol = \"|\"\n",
    "    eval_fn = or_helper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parsing grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyparsing import infixNotation, opAssoc, Keyword, Word, alphas, ParserElement,pyparsing_unicode, nums\n",
    "\n",
    "NOT = Keyword(\"not\")\n",
    "AND = Keyword(\"and\")\n",
    "OR = Keyword(\"or\")\n",
    "token = Word(pyparsing_unicode.printables)\n",
    "token.setParseAction(BoolRetrievalOperand).setName(\"token\")\n",
    "\n",
    "boolOperand = token\n",
    "boolOperand.setName(\"bool_operand\")\n",
    "\n",
    "# define expression, based on expression operand and\n",
    "# list of operations in precedence order\n",
    "boolExpr = infixNotation(\n",
    "    boolOperand,\n",
    "    [\n",
    "        (NOT, 1, opAssoc.RIGHT,BoolRetrievalNot),\n",
    "        (AND, 2, opAssoc.LEFT,BoolRetrievalAnd),\n",
    "        (OR, 2, opAssoc.LEFT,BoolRetrievalOr),\n",
    "    ],\n",
    ").setName(\"boolean_expression\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalNot[['not', one]]\n",
      "one\n",
      "Creating BoolRetrievalNot[['not', ~one]]\n",
      "~one\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2), (1, 1)]\n",
      "Processing ~one\n",
      "[(2, 0)]\n",
      "Processing ~~one\n",
      "[(0, 0), (1, 0)]\n",
      "Query: not not one \n",
      " ~~one = [(0, 0), (1, 0)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2), (1, 1)]\n",
      "Query: one \n",
      " one = [(0, 2), (1, 1)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1)]\n",
      "Query: tree \n",
      " tree = [(2, 1)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[one, 'and', tree]]\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2), (1, 1)]\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1)]\n",
      "Processing (one & tree)\n",
      "[]\n",
      "Query: one and tree \n",
      " (one & tree) = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[one, 'and', two]]\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2), (1, 1)]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3), (2, 1)]\n",
      "Processing (one & two)\n",
      "[(1, 0)]\n",
      "Query: one and two \n",
      " (one & two) = [(1, 0)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[two, 'and', two]]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3), (2, 1)]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3), (2, 1)]\n",
      "Processing (two & two)\n",
      "[(1, 0), (2, 0)]\n",
      "Query: two and two \n",
      " (two & two) = [(1, 0), (2, 0)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalOperand['(two']\n",
      "Creating BoolRetrievalOperand['one)']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[two, 'and', (two, 'and', one)]]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3), (2, 1)]\n",
      "Processing BoolRetrievalOperand (two\n",
      "[]\n",
      "Processing BoolRetrievalOperand one)\n",
      "[]\n",
      "Processing (two & (two & one))\n",
      "[]\n",
      "Query: two and (two and one) \n",
      " (two & (two & one)) = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[one, 'and', tree, 'and', two]]\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2), (1, 1)]\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1)]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3), (2, 1)]\n",
      "Processing (one & tree & two)\n",
      "[]\n",
      "Query: one and tree and two \n",
      " (one & tree & two) = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Creating BoolRetrievalBinOp |\n",
      "[[one, 'or', tree]]\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2), (1, 1)]\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1)]\n",
      "[(0, 2), (1, 1)]\n",
      "Processing (one | tree)\n",
      "[(0, 0), (1, 0), (2, 0)]\n",
      "Query: one or tree \n",
      " (one | tree) = [(0, 0), (1, 0), (2, 0)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['(one']\n",
      "Creating BoolRetrievalOperand['tree)']\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[tree), 'and', two]]\n",
      "Creating BoolRetrievalBinOp |\n",
      "[[(one, 'or', (tree) & two)]]\n",
      "Processing BoolRetrievalOperand (one\n",
      "[]\n",
      "Processing BoolRetrievalOperand tree)\n",
      "[]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3), (2, 1)]\n",
      "Processing (tree) & two)\n",
      "[]\n",
      "[]\n",
      "Processing ((one | (tree) & two))\n",
      "[]\n",
      "Query: (one or tree) and two \n",
      " ((one | (tree) & two)) = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['((one']\n",
      "Creating BoolRetrievalNot[['not', ((one]]\n",
      "((one\n",
      "Creating BoolRetrievalOperand['tree)']\n",
      "Creating BoolRetrievalOperand['two)']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[tree), 'and', two)]]\n",
      "Creating BoolRetrievalBinOp |\n",
      "[[~((one, 'or', (tree) & two))]]\n",
      "Processing BoolRetrievalOperand ((one\n",
      "[]\n",
      "Processing ~((one\n",
      "[(0, 0), (1, 0), (2, 0)]\n",
      "Processing BoolRetrievalOperand tree)\n",
      "[]\n",
      "Processing BoolRetrievalOperand two)\n",
      "[]\n",
      "Processing (tree) & two))\n",
      "[]\n",
      "[(0, 0), (1, 0), (2, 0)]\n",
      "Processing (~((one | (tree) & two)))\n",
      "[(0, 0), (1, 0), (2, 0)]\n",
      "Query: not((one or tree) and two) \n",
      " (~((one | (tree) & two))) = [(0, 0), (1, 0), (2, 0)] \n",
      " test \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    (\"not not one\",True),\n",
    "    (\"one\", True),\n",
    "    (\"tree\", True),\n",
    "    (\"one and tree\",True),\n",
    "    (\"one and two\",True),\n",
    "    (\"two and two\",True),\n",
    "    (\"two and (two and one)\",True),\n",
    "    (\"one and tree and two\",True),\n",
    "    (\"one or tree\",True),\n",
    "    (\"(one or tree) and two\",True),\n",
    "    (\"not((one or tree) and two)\",True),\n",
    "]\n",
    "\n",
    "\n",
    "for test_string, expected in tests:\n",
    "    res = boolExpr.parseString(test_string)[0]\n",
    "    success = \"test\"#\"PASS\" if bool(res) == expected else \"FAIL\"\n",
    "    print(\"Query: \"+test_string, \"\\n\", res, \"=\", str(res.process()), \"\\n\", success, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found documents:  3\n",
      "{0: 'galileo_galilei.txt', 1: 'isaac_newton.txt', 2: 'leonardo_da_vinci.txt'}\n",
      "Creating BoolRetrievalOperand['inventor']\n",
      "Creating BoolRetrievalOperand['Renaissance']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[inventor, 'and', Renaissance]]\n",
      "Processing BoolRetrievalOperand inventor\n",
      "[(2, 1)]\n",
      "Processing BoolRetrievalOperand Renaissance\n",
      "[]\n",
      "Processing (inventor & Renaissance)\n",
      "[]\n",
      "[]\n",
      "Creating BoolRetrievalOperand['inventor']\n",
      "Creating BoolRetrievalOperand['renaissance']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[inventor, 'and', renaissance]]\n",
      "Processing BoolRetrievalOperand inventor\n",
      "[(2, 1)]\n",
      "Processing BoolRetrievalOperand renaissance\n",
      "[(0, 1), (2, 8)]\n",
      "Processing (inventor & renaissance)\n",
      "[(2, 0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0)]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentDir=\"people\"\n",
    "tokenized_documents = prepare_dataset(documentDir)\n",
    "# print(tokenized_documents)\n",
    "token_docid, doc_ids = get_token_doc_id_pairs(documentDir)\n",
    "sorted_token_docid = sorted(token_docid, key=itemgetter(0))\n",
    "merged_tokens_in_doc = sqash_token_in_doc(sorted_token_docid)\n",
    "\n",
    "dictionary = defaultdict(lambda: (0, 0)) # term : doc_freq, tot freq\n",
    "postings = defaultdict(lambda: []) # term: doc_ids, doc_freq\n",
    "\n",
    "print(doc_ids)\n",
    "for token, doc_id, doc_freq in merged_tokens_in_doc:\n",
    "    dictionary[token] = (dictionary[token][0]+1, dictionary[token][1\n",
    "    ]+doc_freq)\n",
    "\n",
    "# usually implemented as linked lists\n",
    "for token, doc_id, doc_freq in merged_tokens_in_doc:\n",
    "    postings[token].append((doc_id, doc_freq)) \n",
    "\n",
    "query = \"inventor and Renaissance\"\n",
    "res = boolExpr.parseString(query)[0]\n",
    "print(res.process())\n",
    "\n",
    "query = \"inventor and renaissance\"\n",
    "res = boolExpr.parseString(query)[0]\n",
    "res.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found documents:  27\n",
      "{0: '1. Графи.md', 1: '10. Нормални форми на БД.md', 2: '11. C++ basics.md', 3: '12. ООП.md', 4: '13. Структури от данни и алгоритми.md', 5: '14. Софтуерно инженерство.md', 6: '15. Разпределени системи.md', 7: '16. GUI.md', 8: '17. QA.md', 9: '18. Софтуерни архитектури.md', 10: '19. Requirenments engineering.md', 11: '2. Булеви функции.md', 12: '20. ПИСС.md', 13: '21. Project management.md', 14: '22. XML.md', 15: '23. Taylor series.md', 16: '24. Integrals.md', 17: '25. Vector space and algebra.md', 18: '26. Polynomials.md', 19: '27. Statistics.md', 20: '3. Автомати.md', 21: '4. Граматики.md', 22: '5. CPU.md', 23: '6. Memory pages and segments, interrupts.md', 24: '7. Processes in OS.md', 25: '8. Networking.md', 26: '9. Релационен модел.md'}\n",
      "Creating BoolRetrievalOperand['algorithm']\n",
      "Creating BoolRetrievalOperand['algorithms']\n",
      "Creating BoolRetrievalBinOp |\n",
      "[[algorithm, 'or', algorithms]]\n",
      "Processing BoolRetrievalOperand algorithm\n",
      "[]\n",
      "Processing BoolRetrievalOperand algorithms\n",
      "[]\n",
      "[]\n",
      "Processing (algorithm | algorithms)\n",
      "[]\n",
      "[]\n",
      "Creating BoolRetrievalOperand['алгоритъм']\n",
      "Creating BoolRetrievalOperand['алгоритми']\n",
      "Creating BoolRetrievalBinOp |\n",
      "[[алгоритъм, 'or', алгоритми]]\n",
      "Processing BoolRetrievalOperand алгоритъм\n",
      "[(1, 1), (4, 1), (17, 1), (18, 2), (20, 1)]\n",
      "Processing BoolRetrievalOperand алгоритми\n",
      "[(4, 4)]\n",
      "[(1, 1), (4, 1), (17, 1), (18, 2), (20, 1)]\n",
      "Processing (алгоритъм | алгоритми)\n",
      "[(1, 0), (4, 0), (17, 0), (18, 0), (20, 0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0), (4, 0), (17, 0), (18, 0), (20, 0)]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentDir=r\"C:\\Users\\Victor\\OneDrive\\Documents\\Second brain\\2. Knowledge base\\Finals\\Themes\"\n",
    "tokenized_documents = prepare_dataset(documentDir)\n",
    "# print(tokenized_documents)\n",
    "token_docid, doc_ids = get_token_doc_id_pairs(documentDir)\n",
    "sorted_token_docid = sorted(token_docid, key=itemgetter(0))\n",
    "merged_tokens_in_doc = sqash_token_in_doc(sorted_token_docid)\n",
    "\n",
    "dictionary = defaultdict(lambda: (0, 0)) # term : doc_freq, tot freq\n",
    "postings = defaultdict(lambda: []) # term: doc_ids, doc_freq\n",
    "\n",
    "print(doc_ids)\n",
    "for token, doc_id, doc_freq in merged_tokens_in_doc:\n",
    "    dictionary[token] = (dictionary[token][0]+1, dictionary[token][1\n",
    "    ]+doc_freq)\n",
    "\n",
    "# usually implemented as linked lists\n",
    "for token, doc_id, doc_freq in merged_tokens_in_doc:\n",
    "    postings[token].append((doc_id, doc_freq)) \n",
    "\n",
    "query = \"algorithm or algorithms\"\n",
    "res = boolExpr.parseString(query)[0]\n",
    "print(res.process())\n",
    "\n",
    "query = \"алгоритъм or алгоритми\"\n",
    "res = boolExpr.parseString(query)[0]\n",
    "res.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'World', '!']\n",
      "['Bonjour', ',', 'Monde', '!']\n",
      "['Hola', ',', 'Mundo', '!']\n",
      "['Hallo', ',', 'Welt', '!']\n"
     ]
    }
   ],
   "source": [
    "greet = Word(alphas) + \",\" + Word(alphas) + \"!\"\n",
    "for greeting_str in [\n",
    "            \"Hello, World!\",\n",
    "            \"Bonjour, Monde!\",\n",
    "            \"Hola, Mundo!\",\n",
    "            \"Hallo, Welt!\",\n",
    "        ]:\n",
    "    greeting = greet.parse_string(greeting_str)\n",
    "    print(greeting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BoolRetrievalWildcard['asdasda', '*']\n",
      "Processing BoolRetrievalWildcard asdasda*\n",
      "[]\n",
      "Query: asdasda* \n",
      " asdasda* = [[]] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Processing BoolRetrievalOperand one\n",
      "[]\n",
      "Query: one \n",
      " one = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Processing BoolRetrievalOperand tree\n",
      "[]\n",
      "Query: tree \n",
      " tree = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalProximity['one', '/', '4', 'tree']\n",
      "Processing BoolRetrievalProximity one \\4 tree\n",
      "Query: one /4 tree \n",
      " TODO = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalProximity['one', '/', '4', 'tree']\n",
      "Processing BoolRetrievalProximity one \\4 tree\n",
      "Query: one /4 tree /5 five \n",
      " TODO = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalSentence['/s', 'one', 'tree', 'five', 'six']\n",
      "Processing BoolRetrievalSentence ['one', 'tree', 'five', 'six']\n",
      "Query: /s one tree five six \n",
      " TODO = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalWildcard['one', '*']\n",
      "Creating BoolRetrievalSentence['/s', 'what', 'this']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[one*, 'and', TODO]]\n",
      "Creating BoolRetrievalProximity['will', '/', '3', 'be']\n",
      "Creating BoolRetrievalBinOp |\n",
      "[[(one* & TODO), 'or', TODO]]\n",
      "Processing BoolRetrievalWildcard one*\n",
      "[]\n",
      "Processing BoolRetrievalSentence ['what', 'this']\n",
      "Processing (one* & TODO)\n",
      "[]\n",
      "Processing BoolRetrievalProximity will \\3 be\n",
      "[]\n",
      "Processing ((one* & TODO) | TODO)\n",
      "[]\n",
      "Query: (one* and (/s what this)) or (will /3 be) \n",
      " ((one* & TODO) | TODO) = [] \n",
      " test \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BoolRetrievalWildcard:\n",
    "    def __init__(self, t):\n",
    "        self.label = t[0]\n",
    "        print(\"Creating BoolRetrievalWildcard\" + str(t))\n",
    "    \n",
    "    def process(self) -> list:\n",
    "        print(\"Processing BoolRetrievalWildcard \" + str(self))\n",
    "        self.value = postings[ self.label]\n",
    "        print(self.value)\n",
    "        return [self.value]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.label+\"*\"\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "class BoolRetrievalProximity:\n",
    "    def __init__(self, t):\n",
    "        self.A = t[0]\n",
    "        self.positions = t[2]\n",
    "        self.B = t[3]\n",
    "        print(\"Creating BoolRetrievalProximity\" + str(t))\n",
    "    \n",
    "    def process(self) -> list:\n",
    "        print(\"Processing BoolRetrievalProximity \" + self.A + \" \\\\\" + self.positions + \" \" + self.B)\n",
    "       # self.value = postings[ self.label]\n",
    "      #  print(self.value)\n",
    "        return []\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"\" + self.A + \" \\\\\" + self.positions + \" \" + self.B\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "class BoolRetrievalSentence:\n",
    "    def __init__(self, t):\n",
    "        self.words = t[1:]\n",
    "        print(\"Creating BoolRetrievalSentence\" + str(t))\n",
    "    \n",
    "    def process(self) -> list:\n",
    "        print(\"Processing BoolRetrievalSentence \" + str(self.words))\n",
    "       # self.value = postings[ self.label]\n",
    "      #  print(self.value)\n",
    "        return []\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"/s\"+str(self.words)\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "token = Word(alphas)\n",
    "token.setParseAction(BoolRetrievalOperand).setName(\"token\")\n",
    "\n",
    "wildcard =  Word(alphas) + \"*\"\n",
    "wildcard.setParseAction(BoolRetrievalWildcard).setName(\"wildcard\")\n",
    "\n",
    "#   Word(alphas) + (\"/\" + Word(nums) + Word(alphas))[1,...]\n",
    "proximity = Word(alphas) + \"/\" + Word(nums) + Word(alphas)\n",
    "proximity.setParseAction(BoolRetrievalProximity).setName(\"proximity\")\n",
    "\n",
    "sentence = \"/s\" + Word(alphas)[1,...]\n",
    "sentence.setParseAction(BoolRetrievalSentence).setName(\"proximity\")\n",
    "\n",
    "boolOperand = sentence | proximity | wildcard | token\n",
    "\n",
    "# define expression, based on expression operand and\n",
    "# list of operations in precedence order\n",
    "boolExpr = infixNotation(\n",
    "    boolOperand,\n",
    "    [\n",
    "        (NOT, 1, opAssoc.RIGHT,BoolRetrievalNot),\n",
    "        (AND, 2, opAssoc.LEFT,BoolRetrievalAnd),\n",
    "        (OR, 2, opAssoc.LEFT,BoolRetrievalOr),\n",
    "    ],\n",
    ")\n",
    "\n",
    "tests = [\n",
    "    (\"asdasda*\",True),\n",
    "    (\"one\", True),\n",
    "    (\"tree\", True),\n",
    "    (\"one /4 tree\", True),\n",
    "    (\"one /4 tree /5 five\", True),\n",
    "    (\"/s one tree five six\", True),\n",
    "   # (\"one or tree*\",True),\n",
    "    (\"(one* and (/s what this)) or (will /3 be)\",True),\n",
    "]\n",
    "\n",
    "\n",
    "for test_string, expected in tests:\n",
    "    res = boolExpr.parseString(test_string)[0]\n",
    "    success = \"test\"#\"PASS\" if bool(res) == expected else \"FAIL\"\n",
    "    print(\"Query: \"+test_string, \"\\n\", res, \"=\", str(res.process()), \"\\n\", success, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6708a22f42bfc2d67753a676f60c57761b9bb23c0c7f2c58114ca823d1c1ffd1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
