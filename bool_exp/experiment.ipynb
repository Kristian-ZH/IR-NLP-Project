{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if in virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def get_base_prefix_compat():\n",
    "    \"\"\"Get base/real prefix, or sys.prefix if there is none.\"\"\"\n",
    "    return getattr(sys, \"base_prefix\", None) or getattr(sys, \"real_prefix\", None) or sys.prefix\n",
    "\n",
    "def in_virtualenv():\n",
    "    return get_base_prefix_compat() != sys.prefix\n",
    "\n",
    "in_virtualenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['China', 'confirms', 'Interpol', 'chief', 'detained']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bbc_news_sentences = [\n",
    "    \"China confirms Interpol chief detained\",\n",
    "    \"Turkish officials believe the Washington Post writer was killed in the Saudi consulate in Istanbul.\",\n",
    "    \"US wedding limousine crash kills 20\",\n",
    "    \"Bulgarian journalist killed in park\",\n",
    "    \"Kanye West deletes social media profiles\",\n",
    "    \"Brazilians vote in polarised election\",\n",
    "    \"Bull kills woman at French festival\",\n",
    "    \"Indonesia to wrap up tsunami search\",\n",
    "    \"Tina Turner reveals wedding night ordeal\",\n",
    "    \"Victory for Trump in Supreme Court battle\",\n",
    "    \"Clashes at German far-right rock concert\",\n",
    "    \"The Walking Dead actor dies aged 76\",\n",
    "    \"Jogger in Netherlands finds lion cub\",\n",
    "    \"Monkey takes the wheel of Indian bus\"\n",
    "]\n",
    "#basic tokenization\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "sample_bbc_news_sentences_tokenized = [tokenizer.tokenize(sent) \n",
    "                            for sent in sample_bbc_news_sentences]\n",
    "sample_bbc_news_sentences_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china', 'confirms', 'interpol', 'chief', 'detained']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bbc_news_sentences_tokenized_lower = [[_t.lower() \n",
    "                                              for _t in _s] \n",
    "                for _s in sample_bbc_news_sentences_tokenized]\n",
    "sample_bbc_news_sentences_tokenized_lower[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '20',\n",
       " '76',\n",
       " 'actor',\n",
       " 'aged',\n",
       " 'at',\n",
       " 'battle',\n",
       " 'believe',\n",
       " 'brazilians',\n",
       " 'bulgarian',\n",
       " 'bull',\n",
       " 'bus',\n",
       " 'chief',\n",
       " 'china',\n",
       " 'clashes',\n",
       " 'concert',\n",
       " 'confirms',\n",
       " 'consulate',\n",
       " 'court',\n",
       " 'crash',\n",
       " 'cub',\n",
       " 'dead',\n",
       " 'deletes',\n",
       " 'detained',\n",
       " 'dies',\n",
       " 'election',\n",
       " 'far-right',\n",
       " 'festival',\n",
       " 'finds',\n",
       " 'for',\n",
       " 'french',\n",
       " 'german',\n",
       " 'in',\n",
       " 'indian',\n",
       " 'indonesia',\n",
       " 'interpol',\n",
       " 'istanbul',\n",
       " 'jogger',\n",
       " 'journalist',\n",
       " 'kanye',\n",
       " 'killed',\n",
       " 'kills',\n",
       " 'limousine',\n",
       " 'lion',\n",
       " 'media',\n",
       " 'monkey',\n",
       " 'netherlands',\n",
       " 'night',\n",
       " 'of',\n",
       " 'officials',\n",
       " 'ordeal',\n",
       " 'park',\n",
       " 'polarised',\n",
       " 'post',\n",
       " 'profiles',\n",
       " 'reveals',\n",
       " 'rock',\n",
       " 'saudi',\n",
       " 'search',\n",
       " 'social',\n",
       " 'supreme',\n",
       " 'takes',\n",
       " 'the',\n",
       " 'tina',\n",
       " 'to',\n",
       " 'trump',\n",
       " 'tsunami',\n",
       " 'turkish',\n",
       " 'turner',\n",
       " 'up',\n",
       " 'us',\n",
       " 'victory',\n",
       " 'vote',\n",
       " 'walking',\n",
       " 'was',\n",
       " 'washington',\n",
       " 'wedding',\n",
       " 'west',\n",
       " 'wheel',\n",
       " 'woman',\n",
       " 'wrap',\n",
       " 'writer'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all unique tokens\n",
    "unique_tokens = set(sum(sample_bbc_news_sentences_tokenized_lower, \n",
    "                        []))\n",
    "unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from string import punctuation\n",
    "import os\n",
    "def preprocess_document(content):\n",
    "    \"\"\"\n",
    "    Returns a list of tokens for a document's content. \n",
    "    Tokens should not contain punctuation and should be lower-cased.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(content)\n",
    "    tokens = []\n",
    "    for _sent in sentences:\n",
    "        sent_tokens = tokenizer.tokenize(_sent)\n",
    "        sent_tokens = [_tok.lower() for _tok in sent_tokens if _tok not in punctuation]\n",
    "        tokens += sent_tokens\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def prepare_dataset(documents_dir):\n",
    "    \"\"\"\n",
    "    Returns list of documents in the documents_dir, where each document is a list of its tokens. \n",
    "    \n",
    "    \"\"\"\n",
    "    tokenized_documents = []\n",
    "    for document in os.listdir(documents_dir):\n",
    "        with open(os.path.join(documents_dir, document), errors='ignore') as outf:\n",
    "            tokenized_documents.append(preprocess_document(outf.read()))\n",
    "    print(\"Found documents: \", len(tokenized_documents))\n",
    "    return tokenized_documents      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found documents:  3\n",
      "[['one', 'one'], ['two', 'one', 'two', 'two'], ['two', 'tree']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(prepare_dataset('test/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import scandir # can be used for easier iteration of documents in a folder\n",
    "# can check is_file() on the objects returned by scan_dir \n",
    "# contain whole document path, so no need to join with the directory\n",
    "\n",
    "def get_token_doc_id_pairs(category_dir):\n",
    "    \"\"\"\n",
    "    Iteratively goes through the documents in the category_dir and constructs/returns:\n",
    "    1. A list of (token, doc_id) tuples\n",
    "    2. A dictionary of doc_id:doc_name\n",
    "    \"\"\"\n",
    "    token_docid = []\n",
    "    doc_ids = {}\n",
    "\n",
    "    for i, document in enumerate(scandir(category_dir)):\n",
    "        if document.is_file():\n",
    "            doc_ids[i] = document\n",
    "            with open(document) as out_fp:\n",
    "                document_tokens = preprocess_document(out_fp.read())\n",
    "                token_docid += [(token, i) for token in document_tokens]\n",
    "    return token_docid, doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: <DirEntry 'one.txt'>, 1: <DirEntry 'one_two.txt'>, 2: <DirEntry 'two_tree.txt'>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('one', 0),\n",
       " ('one', 0),\n",
       " ('two', 1),\n",
       " ('one', 1),\n",
       " ('two', 1),\n",
       " ('two', 1),\n",
       " ('two', 2),\n",
       " ('tree', 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_docid, doc_ids = get_token_doc_id_pairs('test/')\n",
    "print(doc_ids)\n",
    "token_docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 0),\n",
       " ('one', 0),\n",
       " ('one', 1),\n",
       " ('tree', 2),\n",
       " ('two', 1),\n",
       " ('two', 1),\n",
       " ('two', 1),\n",
       " ('two', 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "sorted_token_docid = sorted(token_docid, key=itemgetter(0))\n",
    "sorted_token_docid[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_token_in_doc(sorted_token_docid):\n",
    "    \"\"\"\n",
    "    Returns a list of (token, doc_id, term_freq) tuples from a sorted list of (token, doc_id) list, \n",
    "    where if a token appears n times in a doc_id, we merge it in a tuple (toke, doc_id, n).\n",
    "    \"\"\"\n",
    "    merged_tokens_in_doc = []\n",
    "    for token, doc_id in sorted_token_docid:\n",
    "        if merged_tokens_in_doc:\n",
    "            prev_tok, prev_doc_id, prev_freq = merged_tokens_in_doc[-1]\n",
    "            if prev_tok == token and prev_doc_id == doc_id:     \n",
    "                merged_tokens_in_doc[-1] = (token, doc_id, prev_freq+1)\n",
    "            else:\n",
    "                merged_tokens_in_doc.append((token, doc_id, 1))\n",
    "        else:\n",
    "            merged_tokens_in_doc.append((token, doc_id, 1))\n",
    "    return merged_tokens_in_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 0, 2), ('one', 1, 1), ('tree', 2, 1), ('two', 1, 3), ('two', 2, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_tokens_in_doc = merge_token_in_doc(sorted_token_docid)\n",
    "merged_tokens_in_doc[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dictionary = defaultdict(lambda: (0, 0)) # term : doc_freq, tot freq\n",
    "postings = defaultdict(lambda: []) # term: doc_ids, doc_freq\n",
    "\n",
    "for token, doc_id, doc_freq in merged_tokens_in_doc:\n",
    "    dictionary[token] = (dictionary[token][0]+1, dictionary[token][0]+doc_freq)\n",
    "\n",
    "# usually implemented as linked lists\n",
    "for token, doc_id, doc_freq in merged_tokens_in_doc:\n",
    "    postings[token].append((doc_id, doc_freq)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <DirEntry 'one.txt'>,\n",
       " 1: <DirEntry 'one_two.txt'>,\n",
       " 2: <DirEntry 'two_tree.txt'>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2), (2, 2), (1, 1), (0, 0))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"one\"],dictionary['two'],dictionary['tree'],dictionary['zero']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 2), (1, 1)], [(1, 3), (2, 1)], [(2, 1)], [])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings[\"one\"],postings['two'],postings['tree'],postings['zero']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and query if edit is equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_query(postings, word1, word2):\n",
    "    \"\"\"\n",
    "    merging postings lists of two words\n",
    "    \"\"\"\n",
    "    postings_word1 = postings[word1]\n",
    "    postings_word2 = postings[word2]\n",
    "    \n",
    "    documents_results = []\n",
    "    \n",
    "    postings_ind1, postings_ind2 = 0, 0\n",
    "    while postings_ind1 < len(postings_word1) and postings_ind2 < len(postings_word2):\n",
    "        doc_id1, doc_id2 = postings_word1[postings_ind1][0], postings_word2[postings_ind2][0]\n",
    "        if doc_id1 == doc_id2:\n",
    "            documents_results.append(doc_id1)\n",
    "            postings_ind1 += 1\n",
    "            postings_ind2 += 1\n",
    "        elif doc_id1 < doc_id2:\n",
    "            postings_ind1 += 1\n",
    "        elif doc_id1 > doc_id2:\n",
    "            postings_ind2 += 1\n",
    "    return documents_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check or query for correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_query(postings, word1, word2):\n",
    "    \"\"\"\n",
    "    merging postings lists of two words\n",
    "    \"\"\"\n",
    "    postings_word1 = postings[word1]\n",
    "    postings_word2 = postings[word2]\n",
    "    \n",
    "    documents_results = []\n",
    "    \n",
    "    postings_ind1, postings_ind2 = 0, 0\n",
    "    while postings_ind1 < len(postings_word1) and postings_ind2 < len(postings_word2):\n",
    "        doc_id1, doc_id2 = postings_word1[postings_ind1][0], postings_word2[postings_ind2][0]\n",
    "        if doc_id1 == doc_id2:\n",
    "            documents_results.append(doc_id1)\n",
    "            postings_ind1 += 1\n",
    "            postings_ind2 += 1\n",
    "        elif doc_id1 < doc_id2:\n",
    "            documents_results.append(doc_id1)\n",
    "            postings_ind1 += 1\n",
    "        elif doc_id1 > doc_id2:\n",
    "            documents_results.append(doc_id2)\n",
    "            postings_ind2 += 1\n",
    "    if postings_ind1 == len(postings_word1):\n",
    "        for i in range(postings_ind2,len(postings_word2)):\n",
    "            documents_results.append(postings_word2[i][0])\n",
    "    if postings_ind2 == len(postings_word2):\n",
    "        for i in range(postings_ind1,len(postings_word1)):\n",
    "            documents_results.append(postings_word1[i][0])\n",
    "    return documents_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "doc_id = and_query(postings, 'one', 'two')\n",
    "print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "doc_id = or_query(postings, 'one', 'two')\n",
    "print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "doc_id = and_query(postings, 'tree', 'two')\n",
    "print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "doc_id = or_query(postings, 'tree', 'two')\n",
    "print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "doc_id = and_query(postings, 'one', 'tree')\n",
    "print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "doc_id = or_query(postings, 'one', 'tree')\n",
    "print(doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "doc_id = or_query(postings, 'tree', 'one')\n",
    "print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (1, 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings[\"one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings[\"tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = True\n",
      "q = False\n",
      "r = True\n",
      "\n",
      "BoolOperand\n",
      "['p']\n",
      "p \n",
      " p = True \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['q']\n",
      "q \n",
      " q = False \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['p']\n",
      "BoolOperand\n",
      "['q']\n",
      "BoolBinOp &\n",
      "[[p, 'and', q]]\n",
      "p and q \n",
      " (p & q) = False \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['p']\n",
      "BoolOperand\n",
      "['q']\n",
      "BoolNot\n",
      "[['not', q]]\n",
      "BoolBinOp &\n",
      "[[p, 'and', ~q]]\n",
      "p and not q \n",
      " (p & ~q) = True \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['p']\n",
      "BoolNot\n",
      "[['not', p]]\n",
      "BoolNot\n",
      "[['not', ~p]]\n",
      "not not p \n",
      " ~~p = True \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['p']\n",
      "BoolOperand\n",
      "['q']\n",
      "BoolBinOp &\n",
      "[[p, 'and', q]]\n",
      "BoolNot\n",
      "[['not', (p & q)]]\n",
      "not(p and q) \n",
      " ~(p & q) = True \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['q']\n",
      "BoolOperand\n",
      "['p']\n",
      "BoolNot\n",
      "[['not', p]]\n",
      "BoolOperand\n",
      "['r']\n",
      "BoolBinOp &\n",
      "[[~p, 'and', r]]\n",
      "BoolBinOp |\n",
      "[[q, 'or', (~p & r)]]\n",
      "q or not p and r \n",
      " (q | (~p & r)) = False \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['q']\n",
      "BoolOperand\n",
      "['p']\n",
      "BoolNot\n",
      "[['not', p]]\n",
      "BoolOperand\n",
      "['r']\n",
      "BoolNot\n",
      "[['not', r]]\n",
      "BoolBinOp |\n",
      "[[q, 'or', ~p, 'or', ~r]]\n",
      "q or not p or not r \n",
      " (q | ~p | ~r) = False \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['q']\n",
      "BoolOperand\n",
      "['p']\n",
      "BoolOperand\n",
      "['r']\n",
      "BoolBinOp &\n",
      "[[p, 'and', r]]\n",
      "BoolNot\n",
      "[['not', (p & r)]]\n",
      "BoolBinOp |\n",
      "[[q, 'or', ~(p & r)]]\n",
      "q or not (p and r) \n",
      " (q | ~(p & r)) = False \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['p']\n",
      "BoolOperand\n",
      "['q']\n",
      "BoolOperand\n",
      "['r']\n",
      "BoolBinOp |\n",
      "[[p, 'or', q, 'or', r]]\n",
      "p or q or r \n",
      " (p | q | r) = True \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['p']\n",
      "BoolOperand\n",
      "['q']\n",
      "BoolOperand\n",
      "['r']\n",
      "BoolOperand\n",
      "['False']\n",
      "BoolBinOp &\n",
      "[[r, 'and', False]]\n",
      "BoolBinOp |\n",
      "[[p, 'or', q, 'or', (r & False)]]\n",
      "p or q or r and False \n",
      " (p | q | (r & False)) = True \n",
      " PASS \n",
      "\n",
      "BoolOperand\n",
      "['p']\n",
      "BoolOperand\n",
      "['q']\n",
      "BoolOperand\n",
      "['r']\n",
      "BoolBinOp |\n",
      "[[p, 'or', q, 'or', r]]\n",
      "BoolOperand\n",
      "['False']\n",
      "BoolBinOp &\n",
      "[[(p | q | r), 'and', False]]\n",
      "(p or q or r) and False \n",
      " ((p | q | r) & False) = False \n",
      " PASS \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# simpleBool.py\n",
    "#\n",
    "# Example of defining a boolean logic parser using\n",
    "# the operatorGrammar helper method in pyparsing.\n",
    "#\n",
    "# In this example, parse actions associated with each\n",
    "# operator expression will \"compile\" the expression\n",
    "# into BoolXXX class instances, which can then\n",
    "# later be evaluated for their boolean value.\n",
    "#\n",
    "# Copyright 2006, by Paul McGuire\n",
    "# Updated 2013-Sep-14 - improved Python 2/3 cross-compatibility\n",
    "# Updated 2021-Sep-27 - removed Py2 compat; added type annotations\n",
    "#\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "from pyparsing import infixNotation, opAssoc, Keyword, Word, alphas, ParserElement\n",
    "\n",
    "ParserElement.enablePackrat()\n",
    "\n",
    "\n",
    "# define classes to be built at parse time, as each matching\n",
    "# expression type is parsed\n",
    "class BoolOperand:\n",
    "    def __init__(self, t):\n",
    "        self.label = t[0]\n",
    "        self.value = eval(t[0])\n",
    "        print(\"BoolOperand\")\n",
    "        print(t)\n",
    "\n",
    "    def __bool__(self) -> bool:\n",
    "        return self.value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.label\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "class BoolNot:\n",
    "    def __init__(self, t):\n",
    "        print(\"BoolNot\")\n",
    "        print(t)\n",
    "        self.arg = t[0][1]\n",
    "\n",
    "    def __bool__(self) -> bool:\n",
    "        v = bool(self.arg)\n",
    "        return not v\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"~\" + str(self.arg)\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "class BoolBinOp:\n",
    "    repr_symbol: str = \"\"\n",
    "    eval_fn: Callable[\n",
    "        [Iterable[bool]], bool\n",
    "    ] = lambda _: False\n",
    "\n",
    "    def __init__(self, t):\n",
    "        print(\"BoolBinOp \"+self.repr_symbol)\n",
    "        print(t)\n",
    "        self.args = t[0][0::2]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        sep = \" %s \" % self.repr_symbol\n",
    "        return \"(\" + sep.join(map(str, self.args)) + \")\"\n",
    "\n",
    "    def __bool__(self) -> bool:\n",
    "        return self.eval_fn(bool(a) for a in self.args)\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "class BoolAnd(BoolBinOp):\n",
    "    repr_symbol = \"&\"\n",
    "    eval_fn = all\n",
    "\n",
    "\n",
    "class BoolOr(BoolBinOp):\n",
    "    repr_symbol = \"|\"\n",
    "    eval_fn = any\n",
    "\n",
    "\n",
    "# define keywords and simple infix notation grammar for boolean\n",
    "# expressions\n",
    "TRUE = Keyword(\"True\")\n",
    "FALSE = Keyword(\"False\")\n",
    "NOT = Keyword(\"not\")\n",
    "AND = Keyword(\"and\")\n",
    "OR = Keyword(\"or\")\n",
    "boolOperand = TRUE | FALSE | Word(alphas, max=1)\n",
    "boolOperand.setParseAction(BoolOperand).setName(\"bool_operand\")\n",
    "\n",
    "# define expression, based on expression operand and\n",
    "# list of operations in precedence order\n",
    "boolExpr = infixNotation(\n",
    "    boolOperand,\n",
    "    [\n",
    "        (NOT, 1, opAssoc.RIGHT, BoolNot),\n",
    "        (AND, 2, opAssoc.LEFT, BoolAnd),\n",
    "        (OR, 2, opAssoc.LEFT, BoolOr),\n",
    "    ],\n",
    ").setName(\"boolean_expression\")\n",
    "\n",
    "\n",
    "p = True\n",
    "q = False\n",
    "r = True\n",
    "tests = [\n",
    "    (\"p\", True),\n",
    "    (\"q\", False),\n",
    "    (\"p and q\", False),\n",
    "    (\"p and not q\", True),\n",
    "    (\"not not p\", True),\n",
    "    (\"not(p and q)\", True),\n",
    "    (\"q or not p and r\", False),\n",
    "    (\"q or not p or not r\", False),\n",
    "    (\"q or not (p and r)\", False),\n",
    "    (\"p or q or r\", True),\n",
    "    (\"p or q or r and False\", True),\n",
    "    (\"(p or q or r) and False\", False),\n",
    "]\n",
    "\n",
    "print(\"p =\", p)\n",
    "print(\"q =\", q)\n",
    "print(\"r =\", r)\n",
    "print()\n",
    "for test_string, expected in tests:\n",
    "    res = boolExpr.parseString(test_string)[0]\n",
    "    success = \"PASS\" if bool(res) == expected else \"FAIL\"\n",
    "    print(test_string, \"\\n\", res, \"=\", bool(res), \"\\n\", success, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_query_list(postings_word1, postings_word2):\n",
    "    \"\"\"\n",
    "    merging postings lists of two words\n",
    "    \"\"\"\n",
    "\n",
    "    documents_results = []\n",
    "    \n",
    "    postings_ind1, postings_ind2 = 0, 0\n",
    "    while postings_ind1 < len(postings_word1) and postings_ind2 < len(postings_word2):\n",
    "        doc_id1, doc_id2 = postings_word1[postings_ind1][0], postings_word2[postings_ind2][0]\n",
    "        if doc_id1 == doc_id2:\n",
    "            documents_results.append((doc_id1,0))\n",
    "            postings_ind1 += 1\n",
    "            postings_ind2 += 1\n",
    "        elif doc_id1 < doc_id2:\n",
    "            postings_ind1 += 1\n",
    "        elif doc_id1 > doc_id2:\n",
    "            postings_ind2 += 1\n",
    "    return documents_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoolRetrievalOperand:\n",
    "    def __init__(self, t):\n",
    "        self.label = t[0]\n",
    "        self.value = postings[t[0]]\n",
    "        print(\"BoolRetrievalOperand\")\n",
    "        print(t)\n",
    "        print(self.value)\n",
    "    \n",
    "    def gimme(self) -> list:\n",
    "        return self.value\n",
    "\n",
    "    def __list__(self) -> list:\n",
    "        return self.value\n",
    "\n",
    "    \n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.label\n",
    "\n",
    "    __repr__ = __str__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoolRetrievalBinOp:\n",
    "    repr_symbol: str = \"\"\n",
    "    eval_fn: Callable[\n",
    "        [Iterable[list]], list\n",
    "    ] = lambda _: []\n",
    "\n",
    "    def __init__(self, t):\n",
    "        print(\"BoolRetrievalBinOp \"+self.repr_symbol)\n",
    "        print(t)\n",
    "        self.args = t[0][0::2]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        sep = \" %s \" % self.repr_symbol\n",
    "        return \"(\" + sep.join(map(str, self.args)) + \")\"\n",
    "\n",
    "   # def __bool__(self) -> bool:\n",
    "    #    return self.eval_fn(bool(a) for a in self.args)\n",
    "\n",
    "    def gimme(self) -> list:\n",
    "        return self.eval_fn(a.gimme() for a in self.args)\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "def wazaa(a,bb) -> list:\n",
    "    print(a,\"yolo\",bb)\n",
    "    b = []\n",
    "    for i in bb:\n",
    "        b.append(i)\n",
    "    prev = b[0]\n",
    "    for i in range(0,len(b)-1):\n",
    "        print(prev)\n",
    "        prev = and_query_list(prev,b[i+1])\n",
    "    return prev\n",
    "\n",
    "class BoolRetrievalAnd(BoolRetrievalBinOp):\n",
    "    repr_symbol = \"&\"\n",
    "    eval_fn = wazaa\n",
    "\n",
    "\n",
    "class BoolRetrievalOr(BoolRetrievalBinOp):\n",
    "    repr_symbol = \"|\"\n",
    "    eval_fn = wazaa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT = Keyword(\"not\")\n",
    "AND = Keyword(\"and\")\n",
    "OR = Keyword(\"or\")\n",
    "boolOperand = Word(alphas)\n",
    "boolOperand.setParseAction(BoolRetrievalOperand).setName(\"bool_operand\")\n",
    "\n",
    "# define expression, based on expression operand and\n",
    "# list of operations in precedence order\n",
    "boolExpr = infixNotation(\n",
    "    boolOperand,\n",
    "    [\n",
    "        (NOT, 1, opAssoc.RIGHT),\n",
    "        (AND, 2, opAssoc.LEFT,BoolRetrievalAnd),\n",
    "        (OR, 2, opAssoc.LEFT),\n",
    "    ],\n",
    ").setName(\"boolean_expression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings[\"one\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = True\n",
      "q = False\n",
      "r = True\n",
      "\n",
      "BoolRetrievalOperand\n",
      "['one']\n",
      "[(0, 2), (1, 1)]\n",
      "one \n",
      " one = [(0, 2), (1, 1)] \n",
      " test \n",
      "\n",
      "BoolRetrievalOperand\n",
      "['tree']\n",
      "[(2, 1)]\n",
      "tree \n",
      " tree = [(2, 1)] \n",
      " test \n",
      "\n",
      "BoolRetrievalOperand\n",
      "['one']\n",
      "[(0, 2), (1, 1)]\n",
      "BoolRetrievalOperand\n",
      "['tree']\n",
      "[(2, 1)]\n",
      "BoolRetrievalBinOp &\n",
      "[[one, 'and', tree]]\n",
      "(one & tree) yolo <generator object BoolRetrievalBinOp.gimme.<locals>.<genexpr> at 0x00000253C6233AC0>\n",
      "[(0, 2), (1, 1)]\n",
      "one and tree \n",
      " (one & tree) = [] \n",
      " test \n",
      "\n",
      "BoolRetrievalOperand\n",
      "['one']\n",
      "[(0, 2), (1, 1)]\n",
      "BoolRetrievalOperand\n",
      "['two']\n",
      "[(1, 3), (2, 1)]\n",
      "BoolRetrievalBinOp &\n",
      "[[one, 'and', two]]\n",
      "(one & two) yolo <generator object BoolRetrievalBinOp.gimme.<locals>.<genexpr> at 0x00000253C6233AC0>\n",
      "[(0, 2), (1, 1)]\n",
      "one and two \n",
      " (one & two) = [(1, 0)] \n",
      " test \n",
      "\n",
      "BoolRetrievalOperand\n",
      "['two']\n",
      "[(1, 3), (2, 1)]\n",
      "BoolRetrievalOperand\n",
      "['two']\n",
      "[(1, 3), (2, 1)]\n",
      "BoolRetrievalBinOp &\n",
      "[[two, 'and', two]]\n",
      "(two & two) yolo <generator object BoolRetrievalBinOp.gimme.<locals>.<genexpr> at 0x00000253C6233AC0>\n",
      "[(1, 3), (2, 1)]\n",
      "two and two \n",
      " (two & two) = [(1, 0), (2, 0)] \n",
      " test \n",
      "\n",
      "BoolRetrievalOperand\n",
      "['two']\n",
      "[(1, 3), (2, 1)]\n",
      "BoolRetrievalOperand\n",
      "['two']\n",
      "[(1, 3), (2, 1)]\n",
      "BoolRetrievalOperand\n",
      "['one']\n",
      "[(0, 2), (1, 1)]\n",
      "BoolRetrievalBinOp &\n",
      "[[two, 'and', one]]\n",
      "BoolRetrievalBinOp &\n",
      "[[two, 'and', (two & one)]]\n",
      "(two & (two & one)) yolo <generator object BoolRetrievalBinOp.gimme.<locals>.<genexpr> at 0x00000253C6233AC0>\n",
      "(two & one) yolo <generator object BoolRetrievalBinOp.gimme.<locals>.<genexpr> at 0x00000253C6233BA0>\n",
      "[(1, 3), (2, 1)]\n",
      "[(1, 3), (2, 1)]\n",
      "two and (two and one) \n",
      " (two & (two & one)) = [(1, 0)] \n",
      " test \n",
      "\n",
      "BoolRetrievalOperand\n",
      "['one']\n",
      "[(0, 2), (1, 1)]\n",
      "BoolRetrievalOperand\n",
      "['tree']\n",
      "[(2, 1)]\n",
      "BoolRetrievalOperand\n",
      "['two']\n",
      "[(1, 3), (2, 1)]\n",
      "BoolRetrievalBinOp &\n",
      "[[one, 'and', tree, 'and', two]]\n",
      "(one & tree & two) yolo <generator object BoolRetrievalBinOp.gimme.<locals>.<genexpr> at 0x00000253C6233AC0>\n",
      "[(0, 2), (1, 1)]\n",
      "[]\n",
      "one and tree and two \n",
      " (one & tree & two) = [] \n",
      " test \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = True\n",
    "q = False\n",
    "r = True\n",
    "one = \"one\"\n",
    "tests = [\n",
    "    (\"one\", True),\n",
    "    (\"tree\", True),\n",
    "    (\"one and tree\",True),\n",
    "    (\"one and two\",True),\n",
    "    (\"two and two\",True),\n",
    "    (\"two and (two and one)\",True),\n",
    "    (\"one and tree and two\",True),\n",
    "]\n",
    "\n",
    "print(\"p =\", p)\n",
    "print(\"q =\", q)\n",
    "print(\"r =\", r)\n",
    "print()\n",
    "for test_string, expected in tests:\n",
    "    res = boolExpr.parseString(test_string)[0]\n",
    "    success = \"test\"#\"PASS\" if bool(res) == expected else \"FAIL\"\n",
    "    print(test_string, \"\\n\", res, \"=\", str(res.gimme()), \"\\n\", success, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6708a22f42bfc2d67753a676f60c57761b9bb23c0c7f2c58114ca823d1c1ffd1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
