{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentDir = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict, Counter\n",
    "from string import punctuation\n",
    "import os\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_document(content):\n",
    "    \n",
    "    sentences = sent_tokenize(content)\n",
    "    tokens = []\n",
    "    for _sent in sentences:\n",
    "        sent_tokens = tokenizer.tokenize(_sent)\n",
    "        sent_tokens = [_tok.lower() for _tok in sent_tokens if _tok not in punctuation]\n",
    "        tokens += sent_tokens\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize_document_with_sentences(content):\n",
    "\n",
    "    sentences = sent_tokenize(content)\n",
    "    tokens = []\n",
    "    for _sent in sentences:\n",
    "        sent_tokens = tokenizer.tokenize(_sent)\n",
    "        sent_tokens = [_tok.lower() for _tok in sent_tokens if _tok not in punctuation.replace(\".\",\"\")]\n",
    "        tokens += sent_tokens\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(punctuation.replace(\".\",\"\"))\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linguistic modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are no linguistic modules yet\n",
    "Possible are:\n",
    "- stemmer\n",
    "- lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(content):\n",
    "    return tokenize_document_with_sentences(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(documents_dir):\n",
    "\n",
    "    tokenized_documents = []\n",
    "    for document in os.listdir(documents_dir):\n",
    "        with open(os.path.join(documents_dir, document), errors='ignore',encoding='utf8') as outf:\n",
    "            tokenized_documents.append(preprocess_document(outf.read()))\n",
    "    print(\"Found documents: \", len(tokenized_documents))\n",
    "    \n",
    "    return tokenized_documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found documents:  3\n",
      "[['.', 'one', '.', 'one'], ['two', 'one', 'two', 'two'], ['two', '.', 'tree']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_documents = prepare_dataset(documentDir)\n",
    "print(tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import scandir # can be used for easier iteration of documents in a folder\n",
    "# can check is_file() on the objects returned by scan_dir \n",
    "# contain whole document path, so no need to join with the directory\n",
    "\n",
    "def get_document_tokens(document_tokens,doc_id):\n",
    "    #res = [(token, i) for token in document_tokens]\n",
    "    res = []\n",
    "    sentence_id = 0\n",
    "    for position,token in enumerate(document_tokens):\n",
    "        if token == \".\":\n",
    "            sentence_id=sentence_id+1\n",
    "        else:\n",
    "            res.append(((token,position,sentence_id),doc_id))\n",
    "\n",
    "   # res=[((token,i),doc_id) for i,token in enumerate(document_tokens)]\n",
    "    return res\n",
    "\n",
    "def get_token_doc_id_pairs(category_dir):\n",
    "\n",
    "    token_docid = []\n",
    "    doc_ids = {}\n",
    "\n",
    "    for i, document in enumerate(scandir(category_dir)):\n",
    "        if document.is_file():\n",
    "            doc_ids[i] = document.name\n",
    "            with open(document,encoding='utf8') as out_fp:\n",
    "                document_tokens = preprocess_document(out_fp.read())\n",
    "                token_docid += get_document_tokens(document_tokens,i)\n",
    "    return token_docid, doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'one.txt', 1: 'one_two.txt', 2: 'two_tree.txt'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('one', 1, 1), 0),\n",
       " (('one', 3, 2), 0),\n",
       " (('two', 0, 0), 1),\n",
       " (('one', 1, 0), 1),\n",
       " (('two', 2, 0), 1),\n",
       " (('two', 3, 0), 1),\n",
       " (('two', 0, 0), 2),\n",
       " (('tree', 2, 1), 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_docid, doc_ids = get_token_doc_id_pairs(documentDir)\n",
    "print(doc_ids)\n",
    "token_docid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by tokens to form the dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 1, 1), 0),\n",
       " (('one', 3, 2), 0),\n",
       " (('one', 1, 0), 1),\n",
       " (('tree', 2, 1), 2),\n",
       " (('two', 0, 0), 1),\n",
       " (('two', 2, 0), 1),\n",
       " (('two', 3, 0), 1),\n",
       " (('two', 0, 0), 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "sorted_token_docid = sorted(token_docid, key=lambda el: el[0][0])\n",
    "sorted_token_docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_token_in_doc(sorted_token_docid):\n",
    "    \"\"\"\n",
    "    Returns a list of (token, doc_id, term_freq) tuples from a sorted list of (token, doc_id) list, \n",
    "    where if a token appears n times in a doc_id, we merge it in a tuple (toke, doc_id, n).\n",
    "    \"\"\"\n",
    "    merged_tokens_in_doc = []\n",
    "    for combined_token, doc_id in sorted_token_docid:\n",
    "        (token,position,sentence) = combined_token\n",
    "        if merged_tokens_in_doc:\n",
    "            prev_tok, prev_doc_id, prev_freq,prev_positions,prev_sentences = merged_tokens_in_doc[-1]\n",
    "            if prev_tok == token and prev_doc_id == doc_id:     \n",
    "                merged_tokens_in_doc[-1] = (token, doc_id, prev_freq+1,prev_positions + [position],prev_sentences+[sentence])\n",
    "            else:\n",
    "                merged_tokens_in_doc.append((token, doc_id, 1,[position],[sentence]))\n",
    "        else:\n",
    "            merged_tokens_in_doc.append((token, doc_id, 1,[position],[sentence]))\n",
    "    return merged_tokens_in_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 0, 2, [1, 3], [1, 2]),\n",
       " ('one', 1, 1, [1], [0]),\n",
       " ('tree', 2, 1, [2], [1]),\n",
       " ('two', 1, 3, [0, 2, 3], [0, 0, 0]),\n",
       " ('two', 2, 1, [0], [0])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_tokens_in_doc = merge_token_in_doc(sorted_token_docid)\n",
    "merged_tokens_in_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dictionary = defaultdict(lambda: (0, 0)) # term : doc_freq, tot freq\n",
    "postings = defaultdict(lambda: []) # term: doc_ids, doc_freq\n",
    "\n",
    "for token, doc_id, doc_freq,positions,sentences in merged_tokens_in_doc:\n",
    "    dictionary[token] = (dictionary[token][0]+1, dictionary[token][1\n",
    "    ]+doc_freq)\n",
    "\n",
    "# usually implemented as linked lists\n",
    "for token, doc_id, doc_freq,positions,sentences in merged_tokens_in_doc:\n",
    "    postings[token].append((doc_id, doc_freq,positions,sentences)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'one.txt', 1: 'one_two.txt', 2: 'two_tree.txt'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3), (2, 4), (1, 1), (0, 0))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"one\"],dictionary['two'],dictionary['tree'],dictionary['zero']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "[(1, 3, [0, 2, 3], [0, 0, 0]), (2, 1, [0], [0])]\n",
      "[(2, 1, [2], [1])]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(postings[\"one\"])\n",
    "print(postings['two'])\n",
    "print(postings['tree'])\n",
    "print(postings['zero'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Opearations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_query(postings_word1, postings_word2):\n",
    " \n",
    "    documents_results = []\n",
    "    \n",
    "    postings_ind1, postings_ind2 = 0, 0\n",
    "    while postings_ind1 < len(postings_word1) and postings_ind2 < len(postings_word2):\n",
    "        doc_id1, doc_id2 = postings_word1[postings_ind1][0], postings_word2[postings_ind2][0]\n",
    "        if doc_id1 == doc_id2:\n",
    "            documents_results.append((doc_id1,0))\n",
    "            postings_ind1 += 1\n",
    "            postings_ind2 += 1\n",
    "        elif doc_id1 < doc_id2:\n",
    "            postings_ind1 += 1\n",
    "        elif doc_id1 > doc_id2:\n",
    "            postings_ind2 += 1\n",
    "    return documents_results\n",
    "\n",
    "def and_multipar(self,lists) -> list:\n",
    "    prev = lists[0]\n",
    "    for i in range(0,len(lists)-1):\n",
    "        prev = and_query(prev,lists[i+1])\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_query(postings_word1, postings_word2):\n",
    "\n",
    "    documents_results = []\n",
    "    \n",
    "    postings_ind1, postings_ind2 = 0, 0\n",
    "    while postings_ind1 < len(postings_word1) and postings_ind2 < len(postings_word2):\n",
    "        doc_id1, doc_id2 = postings_word1[postings_ind1][0], postings_word2[postings_ind2][0]\n",
    "        if doc_id1 == doc_id2:\n",
    "            documents_results.append((doc_id1,0))\n",
    "            postings_ind1 += 1\n",
    "            postings_ind2 += 1\n",
    "        elif doc_id1 < doc_id2:\n",
    "            documents_results.append((doc_id1,0))\n",
    "            postings_ind1 += 1\n",
    "        elif doc_id1 > doc_id2:\n",
    "            documents_results.append((doc_id2,0))\n",
    "            postings_ind2 += 1\n",
    "    if postings_ind1 == len(postings_word1):\n",
    "        for i in range(postings_ind2,len(postings_word2)):\n",
    "            documents_results.append((postings_word2[i][0],0))\n",
    "    if postings_ind2 == len(postings_word2):\n",
    "        for i in range(postings_ind1,len(postings_word1)):\n",
    "            documents_results.append((postings_word1[i][0],0))\n",
    "    return documents_results\n",
    "\n",
    "def or_multipar(self,lists) -> list:\n",
    "    prev = lists[0]\n",
    "    for i in range(0,len(lists)-1):\n",
    "        prev = or_query(prev,lists[i+1])\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_query(postings_word):\n",
    "    document_count = len(tokenized_documents)\n",
    "    documents_results = []\n",
    "\n",
    "    prev = 0\n",
    "    for i in range(0,len(postings_word)):\n",
    "        for not_doc in range(prev,postings_word[i][0]):\n",
    "            documents_results.append((not_doc,0))\n",
    "        prev = postings_word[i][0]+1\n",
    "    for not_doc in range(prev,document_count):\n",
    "        documents_results.append((not_doc,0))\n",
    "    \n",
    "    return documents_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0)]\n",
      "[(0, 0)]\n",
      "[(0, 0), (1, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(not_query(postings[\"one\"]))\n",
    "print(not_query(postings[\"two\"]))\n",
    "print(not_query(postings[\"tree\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parsing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "class BoolRetrievalOperand:\n",
    "    def __init__(self, t):\n",
    "        self.label = t[0]\n",
    "        print(\"Creating BoolRetrievalOperand\" + str(t))\n",
    "    \n",
    "    def process(self) -> list:\n",
    "        print(\"Processing BoolRetrievalOperand \"+ self.label)\n",
    "        self.value = postings[self.label]\n",
    "        print(self.value)\n",
    "        return self.value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.label\n",
    "\n",
    "    __repr__ = __str__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoolRetrievalNot:\n",
    "    def __init__(self, t):\n",
    "        print(\"Creating BoolRetrievalNot\"+str(t))\n",
    "        self.arg = t[0][1]\n",
    "        print(self.arg)\n",
    "\n",
    "\n",
    "    def process(self) -> list:\n",
    "        res = not_query(self.arg.process())\n",
    "        print(\"Processing \"+str(self))\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"~\" + str(self.arg)\n",
    "\n",
    "    __repr__ = __str__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoolRetrievalBinOp:\n",
    "    repr_symbol: str = \"\"\n",
    "    eval_fn: Callable[\n",
    "        [list[list]], list\n",
    "    ] = lambda _: []\n",
    "\n",
    "    def __init__(self, t):\n",
    "        print(\"Creating BoolRetrievalBinOp \"+self.repr_symbol)\n",
    "        print(t)\n",
    "        self.args = t[0][0::2]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        sep = \" %s \" % self.repr_symbol\n",
    "        return \"(\" + sep.join(map(str, self.args)) + \")\"\n",
    "\n",
    "\n",
    "    def process(self) -> list:\n",
    "        res = self.eval_fn([a.process() for a in self.args])\n",
    "        print(\"Processing \"+str(self))\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "class BoolRetrievalAnd(BoolRetrievalBinOp):\n",
    "    repr_symbol = \"&\"\n",
    "    eval_fn = and_multipar\n",
    "\n",
    "\n",
    "class BoolRetrievalOr(BoolRetrievalBinOp):\n",
    "    repr_symbol = \"|\"\n",
    "    eval_fn = or_multipar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parsing grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyparsing import infixNotation, opAssoc, Keyword, Word, alphas, ParserElement,pyparsing_unicode, nums\n",
    "\n",
    "NOT = Keyword(\"not\")\n",
    "AND = Keyword(\"and\")\n",
    "OR = Keyword(\"or\")\n",
    "token = Word(pyparsing_unicode.printables,exclude_chars=punctuation)\n",
    "token.setParseAction(BoolRetrievalOperand).setName(\"token\")\n",
    "\n",
    "boolOperand = token\n",
    "boolOperand.setName(\"bool_operand\")\n",
    "\n",
    "# define expression, based on expression operand and\n",
    "# list of operations in precedence order\n",
    "boolExpr = infixNotation(\n",
    "    boolOperand,\n",
    "    [\n",
    "        (NOT, 1, opAssoc.RIGHT,BoolRetrievalNot),\n",
    "        (AND, 2, opAssoc.LEFT,BoolRetrievalAnd),\n",
    "        (OR, 2, opAssoc.LEFT,BoolRetrievalOr),\n",
    "    ],\n",
    ").setName(\"boolean_expression\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalNot[['not', one]]\n",
      "one\n",
      "Creating BoolRetrievalNot[['not', ~one]]\n",
      "~one\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "Processing ~one\n",
      "[(2, 0)]\n",
      "Processing ~~one\n",
      "[(0, 0), (1, 0)]\n",
      "Query: not not one \n",
      " ~~one = [(0, 0), (1, 0)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "Query: one \n",
      " one = [(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1, [2], [1])]\n",
      "Query: tree \n",
      " tree = [(2, 1, [2], [1])] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[one, 'and', tree]]\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1, [2], [1])]\n",
      "Processing (one & tree)\n",
      "[]\n",
      "Query: one and tree \n",
      " (one & tree) = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[one, 'and', two]]\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3, [0, 2, 3], [0, 0, 0]), (2, 1, [0], [0])]\n",
      "Processing (one & two)\n",
      "[(1, 0)]\n",
      "Query: one and two \n",
      " (one & two) = [(1, 0)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[two, 'and', two]]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3, [0, 2, 3], [0, 0, 0]), (2, 1, [0], [0])]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3, [0, 2, 3], [0, 0, 0]), (2, 1, [0], [0])]\n",
      "Processing (two & two)\n",
      "[(1, 0), (2, 0)]\n",
      "Query: two and two \n",
      " (two & two) = [(1, 0), (2, 0)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[two, 'and', one]]\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[two, 'and', (two & one)]]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3, [0, 2, 3], [0, 0, 0]), (2, 1, [0], [0])]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3, [0, 2, 3], [0, 0, 0]), (2, 1, [0], [0])]\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "Processing (two & one)\n",
      "[(1, 0)]\n",
      "Processing (two & (two & one))\n",
      "[(1, 0)]\n",
      "Query: two and (two and one) \n",
      " (two & (two & one)) = [(1, 0)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[one, 'and', tree, 'and', two]]\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1, [2], [1])]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3, [0, 2, 3], [0, 0, 0]), (2, 1, [0], [0])]\n",
      "Processing (one & tree & two)\n",
      "[]\n",
      "Query: one and tree and two \n",
      " (one & tree & two) = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Creating BoolRetrievalBinOp |\n",
      "[[one, 'or', tree]]\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1, [2], [1])]\n",
      "Processing (one | tree)\n",
      "[(0, 0), (1, 0), (2, 0)]\n",
      "Query: one or tree \n",
      " (one | tree) = [(0, 0), (1, 0), (2, 0)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Creating BoolRetrievalBinOp |\n",
      "[[one, 'or', tree]]\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[(one | tree), 'and', two]]\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1, [2], [1])]\n",
      "Processing (one | tree)\n",
      "[(0, 0), (1, 0), (2, 0)]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3, [0, 2, 3], [0, 0, 0]), (2, 1, [0], [0])]\n",
      "Processing ((one | tree) & two)\n",
      "[(1, 0), (2, 0)]\n",
      "Query: (one or tree) and two \n",
      " ((one | tree) & two) = [(1, 0), (2, 0)] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Creating BoolRetrievalBinOp |\n",
      "[[one, 'or', tree]]\n",
      "Creating BoolRetrievalOperand['two']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[(one | tree), 'and', two]]\n",
      "Creating BoolRetrievalNot[['not', ((one | tree) & two)]]\n",
      "((one | tree) & two)\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1, [2], [1])]\n",
      "Processing (one | tree)\n",
      "[(0, 0), (1, 0), (2, 0)]\n",
      "Processing BoolRetrievalOperand two\n",
      "[(1, 3, [0, 2, 3], [0, 0, 0]), (2, 1, [0], [0])]\n",
      "Processing ((one | tree) & two)\n",
      "[(1, 0), (2, 0)]\n",
      "Processing ~((one | tree) & two)\n",
      "[(0, 0)]\n",
      "Query: not ((one or tree) and two) \n",
      " ~((one | tree) & two) = [(0, 0)] \n",
      " test \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    (\"not not one\",True),\n",
    "    (\"one\", True),\n",
    "    (\"tree\", True),\n",
    "    (\"one and tree\",True),\n",
    "    (\"one and two\",True),\n",
    "    (\"two and two\",True),\n",
    "    (\"two and (two and one)\",True),\n",
    "    (\"one and tree and two\",True),\n",
    "    (\"one or tree\",True),\n",
    "    (\"(one or tree) and two\",True),\n",
    "    (\"not ((one or tree) and two)\",True),\n",
    "]\n",
    "\n",
    "\n",
    "for test_string, expected in tests:\n",
    "    res = boolExpr.parseString(test_string)[0]\n",
    "    success = \"test\"#\"PASS\" if bool(res) == expected else \"FAIL\"\n",
    "    print(\"Query: \"+test_string, \"\\n\", res, \"=\", str(res.process()), \"\\n\", success, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wildcard_words(word):\n",
    "    return [\"one\",\"two\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woldcard_process(word):\n",
    "    wildcard_words = get_wildcard_words(word)\n",
    "    print(wildcard_words[0])\n",
    "    prew = postings[wildcard_words[0]]\n",
    "    for i in range(1,len(wildcard_words)):\n",
    "        prew = or_query(prew,postings[wildcard_words[i]])\n",
    "    return prew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BoolRetrievalWildcard['asdasda', '*']\n",
      "Processing BoolRetrievalWildcard asdasda*\n",
      "one\n",
      "[(0, 0), (1, 0), (2, 0)]\n",
      "Query: asdasda* \n",
      " asdasda* = [[(0, 0), (1, 0), (2, 0)]] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['one']\n",
      "Processing BoolRetrievalOperand one\n",
      "[(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])]\n",
      "Query: one \n",
      " one = [(0, 2, [1, 3], [1, 2]), (1, 1, [1], [0])] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalOperand['tree']\n",
      "Processing BoolRetrievalOperand tree\n",
      "[(2, 1, [2], [1])]\n",
      "Query: tree \n",
      " tree = [(2, 1, [2], [1])] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalProximity['one', '/', '4', 'tree']\n",
      "Processing BoolRetrievalProximity one \\4 tree\n",
      "Query: one /4 tree \n",
      " one /4 tree = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalProximity['one', '/', '4', 'tree']\n",
      "Processing BoolRetrievalProximity one \\4 tree\n",
      "Query: one /4 tree /5 five \n",
      " one /4 tree = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalSentence['/s', 'one', 'tree', 'five', 'six']\n",
      "Processing BoolRetrievalSentence ['one', 'tree', 'five', 'six']\n",
      "['one', 'tree', 'five', 'six']\n",
      "[]\n",
      "Query: /s one tree five six \n",
      " /s r['one', 'tree', 'five', 'six'] = [] \n",
      " test \n",
      "\n",
      "Creating BoolRetrievalWildcard['one', '*']\n",
      "Creating BoolRetrievalSentence['/s', 'one', 'two']\n",
      "Creating BoolRetrievalBinOp &\n",
      "[[one*, 'and', /s r['one', 'two']]]\n",
      "Creating BoolRetrievalProximity['will', '/', '3', 'be']\n",
      "Creating BoolRetrievalBinOp |\n",
      "[[(one* & /s r['one', 'two']), 'or', will /3 be]]\n",
      "Processing BoolRetrievalWildcard one*\n",
      "one\n",
      "[(0, 0), (1, 0), (2, 0)]\n",
      "Processing BoolRetrievalSentence ['one', 'two']\n",
      "['one', 'two']\n",
      "[]\n",
      "Processing (one* & /s r['one', 'two'])\n",
      "[]\n",
      "Processing BoolRetrievalProximity will \\3 be\n",
      "Processing ((one* & /s r['one', 'two']) | will /3 be)\n",
      "[]\n",
      "Query: (one* and (/s one two)) or (will /3 be) \n",
      " ((one* & /s r['one', 'two']) | will /3 be) = [] \n",
      " test \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BoolRetrievalWildcard:\n",
    "    def __init__(self, t):\n",
    "        self.label = t[0]\n",
    "        print(\"Creating BoolRetrievalWildcard\" + str(t))\n",
    "    \n",
    "    def process(self) -> list:\n",
    "        print(\"Processing BoolRetrievalWildcard \" + str(self))\n",
    "        self.value = woldcard_process(self.label)\n",
    "        print(self.value)\n",
    "        return [self.value]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.label+\"*\"\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "class BoolRetrievalProximity:\n",
    "    def __init__(self, t):\n",
    "        self.A = t[0]\n",
    "        self.positions = t[2]\n",
    "        self.B = t[3]\n",
    "        print(\"Creating BoolRetrievalProximity\" + str(t))\n",
    "    \n",
    "    def process(self) -> list:\n",
    "        print(\"Processing BoolRetrievalProximity \" + self.A + \" \\\\\" + self.positions + \" \" + self.B)\n",
    "       # self.value = postings[ self.label]\n",
    "      #  print(self.value)\n",
    "        return []\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"\" + self.A + \" /\" + self.positions + \" \" + self.B\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "def process_sentence(words):\n",
    "    common_docs = and_multipar(\"\",words)\n",
    "    print(common_docs)\n",
    "    return common_docs\n",
    "\n",
    "class BoolRetrievalSentence:\n",
    "    def __init__(self, t):\n",
    "        self.words = t[1:]\n",
    "        print(\"Creating BoolRetrievalSentence\" + str(t))\n",
    "    \n",
    "    def process(self) -> list:\n",
    "        print(\"Processing BoolRetrievalSentence \" + str(self.words))\n",
    "       # self.value = postings[ self.label]\n",
    "\n",
    "        print(self.words)\n",
    "        return process_sentence(self.words)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"/s r\"+str(self.words)\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "token = Word(alphas)\n",
    "token.setParseAction(BoolRetrievalOperand).setName(\"token\")\n",
    "\n",
    "wildcard =  Word(alphas) + \"*\"\n",
    "wildcard.setParseAction(BoolRetrievalWildcard).setName(\"wildcard\")\n",
    "\n",
    "#   Word(alphas) + (\"/\" + Word(nums) + Word(alphas))[1,...]\n",
    "proximity = Word(alphas) + \"/\" + Word(nums) + Word(alphas)\n",
    "proximity.setParseAction(BoolRetrievalProximity).setName(\"proximity\")\n",
    "\n",
    "sentence = \"/s\" + Word(alphas)[1,...]\n",
    "sentence.setParseAction(BoolRetrievalSentence).setName(\"proximity\")\n",
    "\n",
    "boolOperand = sentence | proximity | wildcard | token\n",
    "\n",
    "# define expression, based on expression operand and\n",
    "# list of operations in precedence order\n",
    "boolExpr = infixNotation(\n",
    "    boolOperand,\n",
    "    [\n",
    "        (NOT, 1, opAssoc.RIGHT,BoolRetrievalNot),\n",
    "        (AND, 2, opAssoc.LEFT,BoolRetrievalAnd),\n",
    "        (OR, 2, opAssoc.LEFT,BoolRetrievalOr),\n",
    "    ],\n",
    ")\n",
    "\n",
    "tests = [\n",
    "    (\"asdasda*\",True),\n",
    "    (\"one\", True),\n",
    "    (\"tree\", True),\n",
    "    (\"one /4 tree\", True),\n",
    "    (\"one /4 tree /5 five\", True),\n",
    "    (\"/s one tree five six\", True),\n",
    "   # (\"one or tree*\",True),\n",
    "    (\"(one* and (/s one two)) or (will /3 be)\",True),\n",
    "]\n",
    "\n",
    "\n",
    "for test_string, expected in tests:\n",
    "    res = boolExpr.parseString(test_string)[0]\n",
    "    success = \"test\"#\"PASS\" if bool(res) == expected else \"FAIL\"\n",
    "    print(\"Query: \"+test_string, \"\\n\", res, \"=\", str(res.process()), \"\\n\", success, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6708a22f42bfc2d67753a676f60c57761b9bb23c0c7f2c58114ca823d1c1ffd1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
